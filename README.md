TensorFlow Basics: 

Neural Networks, Gradient Descent, and Automatic Differentiation
This notebook demonstrates neural network implementation using TensorFlow, automatic differentiation with GradientTape, and optimization with Stochastic Gradient Descent (SGD). A great starting point for understanding TensorFlow's core functionalities.


This project demonstrates:

Building Neural Networks: Using the Sequential API and subclassing Model to define custom neural network models.
Gradient Descent: Implementing Stochastic Gradient Descent (SGD) to minimize loss functions.
Automatic Differentiation: Using tf.GradientTape to compute gradients during backpropagation for optimization.
The notebook includes practical examples and step-by-step explanations to help beginners and intermediate users grasp these essential machine learning concepts.


Notebook Sections

Introduction to TensorFlow

Basic operations with TensorFlow, defining constants and variables.
Building Neural Networks

Using the Sequential API to define a fully connected (Dense) layer.
Subclassing Model to create custom neural network models.
Gradient Descent with Automatic Differentiation

Implementing automatic differentiation using tf.GradientTape.
Minimizing a simple loss function using Stochastic Gradient Descent (SGD).
Visualizing Gradient Descent

Plotting the optimization process and observing how x converges to the target value using gradient updates.
